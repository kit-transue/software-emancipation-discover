
    ParaSET Test Subsystem Functional Specification

        What it is 

            The ParaSET Test Subsystem provides ParaSET developers with test facilities to use with their local executables and environments.

        Functional description 

            This project consists of two areas: 

            o Test Verification

            o ParaSET Test Environment.

            Test Verification

            Test Verification is a tool that allows the user to query the current product state (View, Selection, or Focus), record it in the script, and get notified if the state differs when the script plays back. Test Verification combines ParaSET script facilities and verification calls.

            A verification call is a function call to a ParaSET verify function. A verify function takes a previously recorded product state, compares the previous and the current state, and returns true or false depending on the comparison result.

            A verification call may be automatic or manual. A manual verification call is generated by the developer, using a new Verify item on the Browser Debug menu. (This is the internal Debug menu, not the customer-visible Debugger menu.) An automatic verification call is generated while recording a script.

            Verification calls are generated while recording the script, and are applied while playing it back. Verification calls are applied depending on the Verify Mode value. Verify Mode may be either Verify_Auto, Verify_Manual, Verify_All, or Verify_Disable.

            The user applies Test Verification manually while recording the script. The Browser Debug->Verify cascade menu contains View, Selection, and Focus push buttons, and a toggle button Expected Result (true by default).

            If the user chooses one of the Verify menu buttons, the chosen current state and the Expected Result toggle button value get recorded into the script. When the script is executed the recorded state is compared with the current state. If the result is not equal to the recorded Expected Result toggle button value, the script fails. An automatic verification call always has a true Expected Result.

            The user applies verification manually as many times as he/she wants. On each verification ParaSET generates a character string with the verification call including verification information and writes it as the next line in the recording script. Verification calls are numbered in order starting from 1. A verification call causes the script to exit if the verification result is not the same as the expected result.

            ParaSET Test Environment

            The ParaSET Test Environment (PTE) is a tool for enforcing discipline in ParaSET bug and test processing. To ensure that a script is environment-independent, the user needs to create a test.

            A test is a ParaSET project consisting of a test script and a test environment in a specific read-only directory. The directory name is the same as the test name. A test script is a ParaSET script with verification calls embedded. The test environment combines .pdf, .pref, include directory, relevant source files and directory structure.

            Building a test is not a trivial task. Therefore, end users (such as ParaSET developers) may not be able to create the tests necessary to recreate bugs they encounter. Instead, they may create and submit with the bug report just the script that reproduces the problem. It will be the respobsibility of the ParaSET QA Group to create the test out of the script.

            The ParaSET test suite includes environment-independent tests and scripts that may or may not contain verification calls. The script must have comments to specify environmental considerations. Individuals will likely be able to run tests but not plain scripts in their local environment.

            Scripts should be temporary: a given script should either be turned into a test (if it is possible to reproduce the bug) or thrown out, when the problem is fixed.

            Users may copy tests from the system area and create, copy, rename, or delete them in the local area. They may also put tests from the local area into the system area using a new Put Test function.

            A test script should have comments that specify Test Group, Test Project, and testing problems. The test name should contain any corresponding bug number.

            A Current Test is a writable test copy. The Current Test contains one test copy at a time. There is one Current Test in each of the installed, system, or local areas. The user can execute a test script only in the Current Test.

            Running the test  means to copy a test into the Current Test, set the test environment, and execute the test script. The user must not execute the test script on a Master Test copy. The user can not run more than one test at a time.

            Test scripts produce output on the standard output. This output includes a Start Output Message, Script 'line-by-line' Protocol, Verification Output Messages (if any), and End Output Message.

            The Start Output Message contains the script file name, a time stamp, and the script comments. A Verification Output Message contains the script verification call number, a time stamp, the verification result, and the expected result. The End Output Message contains the script file name, a time stamp, the script duration, and the script result (failed/passed).

            The user may run any combination of tests by specifying bug number, test name(s), Test Group(s), or Test Project(s). The user may redirect the output of a particular test into the separate file in the specified directory. The file name is concatenated with test name and .out extension. If the user does not specify the output directory name, the unique directory name is generated using the user name and timestamp.

            The user can optionally produce a list of failed Tests.

            Tests fall into one of three groups: Smoke tests,  Regression, and Bugs.

            The Smoke test group is a suite of scripts covering the XIP, TTT, and ParaTutor demos, and are applied to a newly-built ParaSET executable to quickly check its integrity.

            The  Regression group contains scripts for fixed bugs and scripts that do not have a bug number. Regression group tests run the same way as the Bug group tests, as described below.

            The  Bugs group contains Tests for open (not-yet-fixed) bugs. The ParaSET bugtool contains a corresponding bug report for each test, and the test directory name contains this bug number. In addition, a test script must include comments that specify the project to which the test belongs, and that describe the testing problems. Bug tests are divided by projects the same way bugs are.

            Tests  are added using the Debug->Create Test menu item. Tests for newly-opened bugs are added to the Bugs group using the Debug->Add Bug Test menu item.

            When the user creates a new test script, ParaSET requests the corresponding bug number (if any) or Test Group (Smoke, Regression), Test comment, and ParaSET command line options (if any).

            When the user chooses the Browser Debug->Add Bug Test menu item, ParaSET asks the user to fill in the New Bug dialogbox, opens a new bug in the bugtool, and starts a new script.

        Justification 

        Test Verification combined with ParaSET scriptability is a powerful tool to test ParaSET and to create "Test Quality Gates".

        The ParaSET Test Environment (PTE) allows the user to create, copy, delete, and run tests in the local area. A Test Administrator may manage tests in the system and install areas.

        Marketing Mandates 

            "Most software professionals think that defects exist because of bugs in the code. A more productive perspective is that defects exist because of bugs in the quality gates - that is, quality gates were not designed or applied sufficiently well to trap all errors." -- Reed Hastings, president and CEO of Pure Software.

        Design description

            There is a new test project directory structure :

            

                       $BASE($SYSBASE)
                            /  
                           /
                         tests
                           |
              ---------------------------------------------
             /      /      \       \          \            \
         Bugs  Regression  Smoke  current_test  outdirs ... scripts
         /...\                                             /    \
        /                                                 /      \
    test_xxxx                                      bugxxxx.ccc ...
       |    
  ----------------------------------------------------
 /               \             \           \          \
test.ccc     test.pdf       test.prefs   files    directories


            What it does 

                Test Verification allows a user to verify the correctness of a test script execution.

                The ParaSET Test Subsystem manages tests of different groups and runs them independently of the environment.

                How to use Test Verification.

                    The verification calls get recorded while recording a script and get applied while playing the script back depended on the Verify Mode value. 

                    Here are the Verify Mode values: 

                    o Verify_All -- Apply both automatic and manual verification.

                    o Verify_Manual -- Apply only manual verification. (Default.)

                    o Verify_Auto -- Apply only automatic verification.

                    o Verify_Disable -- Disable all verification.

                    Verify_Manual is the default mode. The user changes theVerify Mode value with a new Browser Debug->Verify Mode cascade menu, or by using the paraset -verify_mode command line option with one of the values specified above.

                    When a test script is recorded with  Verify_Auto, the user may play it back with a different Verify Mode value.

                    The user may change the Verify Mode value from inside the test script source by inserting the line:

                    

                   cmd_set_verify_mode ( mode );              where 

                    mode is 0, 1, 2, 3 for Verify_All, Verify_Manual, Verify_Auto and Verify_Disable respectively.

                    Manual Verification

                        The user records verification calls by using a new Browser Debug->Verify cascade menu. This menu has View, Selection, and Focus push buttons, and an Expected Result toggle button.  The push buttons define what to verify: current view, current selection, or current focus respectively.

                        The  Expected Result toggle button value defines the correct comparison result between recorded and current information while playing the script back. The Expected Result toggle button value is true by default. The user needs to toggle it to false each time when it is needed.

                    Automatic Verification

                        The user turns on automatic verification by setting the Verify Mode value to Verify_All or Verify_Auto. After this verification calls get recorded and applied automatically.

                How to use the ParaSET Test Environment (PTE)

                    The ParaSET Test Environment (PTE) includes the following utilities: 

                    o New Test 

                    o Copy Test

                    o Delete Test

                    o Put Test

                    o Run Test

                    The user starts the PTE Utilities either interactively using a new Browser Debug->Test cascade menu, or in batch mode using Unix shell scripts named ptest_new, ptest_delete, ptest_put, ptest_copy and ptest_run.

                    The current implementation provides only the scripts.

                    A test name must start with "test".  The PTE Copy utility provides name checking to reject wrong or already-used names in the target area.

                    The user may browse test Projects in the ParaSET browser if the column filter matching field specifies the string 'test*'.

                Create a New Test

                

       The user creates a new Test using the ptest_new shell command :
           
                  ptest_new the_current_test         
         

                ...where the_current_test is the current_test parent directory ($BASE/test by default).

                The New Test utility cleans the current_test project directory. The user must provide the correct local test environment to execute the test script. This includes a test.pdf file, test.prefs file, any other relevant files and directories.

                Copy a Test

                The user copies a test with the ptest_copy shell script:

                

                  ptest_copy source_test target_parent        

                where: 

                source_test is the existing test Project.

                target_parent is a directory pathname where the copy is created. target_parent must not be the source_test directory name.

                Run Tests

                The user runs one or more tests with the ptest_run shell script:

                

                  ptest_run [-f input_fname] [test1,...,testn]
                            [-o output_directory]
                            [-false failed_filename]
                            [-g test_group1] ... [-g test_groupN]
                            [-e source_test]           





                where:

                input_fname is a file specifying the test names to run. For instance, file failed_tests_030993 might contain test names test_bug1567, test_bug4605, and test_ttt.

                test1 ... testN are the tests to run.

                output_directory is the directory where the test output protocol files are created ($BASE/test/outdir by default).

                failed_filename is the file for names of failed tests.

                test_group1...test_groupN are the test Groups (Bugs, Smoke or Regression).

                source_test is the Root Test Project ($SYSBASE/install/test by default).

            If no other input parameters are specified, then all tests from all groups of the Root Test Project are run.

            Delete a Test

            The user deletes a test with the test_delete shell script:

            

                  test_delete   name1, ..., name2     
         

            ...where:

            name1, ... , name2 constitute the test name list.

            The user may delete tests in the local area only.

            Put a Test

            The user puts a test into the system area with the test_put shell script:

            

                  test_put    name, group         
              

            where:

            name is the test name.

            group is either the Bugs, Smoke or Regression Test group.

            There is no RCS functionality for tests in this implementation. Put is implemented as a special case for Copy. The user can put an arbitrary test from the current directory into the specified test group of the system area. If the test with the specified name already exists in the specified test group, then it gets overwritten; if it does not exist, then it gets created.

        Environmental considerations 

            Performance

                <List any significant figures relating to memory, disk space, time, etc:

                Memory: 

                Disk Space:

                Time: 

            Scalability 

                <How does this functionality hold up as the demands on it increase?> 

        What does it take 

            Tasks 

            Time 

            Money 

            Resources 

            Ordering 
